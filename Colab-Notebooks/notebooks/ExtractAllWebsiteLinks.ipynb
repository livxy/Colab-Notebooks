{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlQcyZF0TjPqdzR7wY7iaI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/livxy/Colab-Notebooks/blob/main/Colab-Notebooks/notebooks/ExtractAllWebsiteLinks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0MBWKuik4J4",
        "outputId": "d6c72f70-2a37-4f55-9231-dbe26a129ce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (2.23.0)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.7/dist-packages (0.0.1)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (0.4.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests) (2021.10.8)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from bs4) (4.6.3)\n"
          ]
        }
      ],
      "source": [
        "#Let's install the dependencies:\n",
        "!pip3 install requests bs4 colorama"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import modules\n",
        "import requests\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import colorama"
      ],
      "metadata": {
        "id": "1SAYSU1Pk_kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init the colorama module\n",
        "colorama.init()\n",
        "GREEN = colorama.Fore.GREEN\n",
        "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
        "RESET = colorama.Fore.RESET\n",
        "YELLOW = colorama.Fore.YELLOW"
      ],
      "metadata": {
        "id": "S-JltHOvlHOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize the set of links (unique links)\n",
        "internal_urls = set()\n",
        "external_urls = set()"
      ],
      "metadata": {
        "id": "fokPvt7VlIrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to validate URLs:\n",
        "def is_valid(url):\n",
        "    \"\"\"\n",
        "    Checks whether `url` is a valid URL.\n",
        "    \"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    return bool(parsed.netloc) and bool(parsed.scheme)"
      ],
      "metadata": {
        "id": "Drd3XBGslLyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Function to return all the valid URLs of a webpage\n",
        "def get_all_website_links(url):\n",
        "    \"\"\"\n",
        "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
        "    \"\"\"\n",
        "    # all URLs of `url`\n",
        "    urls = set()\n",
        "    # domain name of the URL without the protocol\n",
        "    domain_name = urlparse(url).netloc\n",
        "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
        "    #Get all HTML a tags (anchor tags that contains all the links of the web page):\n",
        "    for a_tag in soup.findAll(\"a\"):\n",
        "        href = a_tag.attrs.get(\"href\")\n",
        "        if href == \"\" or href is None:\n",
        "            # href empty tag\n",
        "            continue\n",
        "        # join the URL if it's relative (not absolute link)\n",
        "        href = urljoin(url, href)\n",
        "        parsed_href = urlparse(href)\n",
        "        # remove URL GET parameters, URL fragments, etc.\n",
        "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
        "        if not is_valid(href):\n",
        "            # not a valid URL\n",
        "            continue\n",
        "        if href in internal_urls:\n",
        "            # already in the set\n",
        "            continue\n",
        "        if domain_name not in href:\n",
        "            # external link\n",
        "            if href not in external_urls:\n",
        "                print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
        "                external_urls.add(href)\n",
        "            continue\n",
        "        print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
        "        urls.add(href)\n",
        "        internal_urls.add(href)\n",
        "    return urls\n"
      ],
      "metadata": {
        "id": "7A2cw5uqlTbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# number of urls visited so far will be stored here\n",
        "total_urls_visited = 0\n",
        "\n",
        "def crawl(url, max_urls=30):\n",
        "    \"\"\"\n",
        "    Crawls a web page and extracts all links.\n",
        "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
        "    params:\n",
        "        max_urls (int): number of max urls to crawl, default is 30.\n",
        "    \"\"\"\n",
        "    global total_urls_visited\n",
        "    total_urls_visited += 1\n",
        "    print(f\"{YELLOW}[*] Crawling: {url}{RESET}\")\n",
        "    links = get_all_website_links(url)\n",
        "    for link in links:\n",
        "        if total_urls_visited > max_urls:\n",
        "            break\n",
        "        crawl(link, max_urls=max_urls)"
      ],
      "metadata": {
        "id": "8UgSW0UWmQcb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test time!\n",
        "if __name__ == \"__main__\":\n",
        "    crawl(\"https://benny.fun/\")\n",
        "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
        "    print(\"[+] Total External links:\", len(external_urls))\n",
        "    print(\"[+] Total URLs:\", len(external_urls) + len(internal_urls))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSDvED0CmT7z",
        "outputId": "2004e8fd-5339-43f4-e599-dfbe54938bf9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Crawling: https://static.bigideasmath.com\n",
            "[+] Total Internal links: 0\n",
            "[+] Total External links: 0\n",
            "[+] Total URLs: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from urllib.parse import urlparse, urljoin\n",
        "from bs4 import BeautifulSoup\n",
        "import colorama\n",
        "\n",
        "# init the colorama module\n",
        "colorama.init()\n",
        "\n",
        "GREEN = colorama.Fore.GREEN\n",
        "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
        "RESET = colorama.Fore.RESET\n",
        "YELLOW = colorama.Fore.YELLOW\n",
        "\n",
        "# initialize the set of links (unique links)\n",
        "internal_urls = set()\n",
        "external_urls = set()\n",
        "\n",
        "total_urls_visited = 0\n",
        "\n",
        "\n",
        "def is_valid(url):\n",
        "    \"\"\"\n",
        "    Checks whether `url` is a valid URL.\n",
        "    \"\"\"\n",
        "    parsed = urlparse(url)\n",
        "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
        "\n",
        "\n",
        "def get_all_website_links(url):\n",
        "    \"\"\"\n",
        "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
        "    \"\"\"\n",
        "    # all URLs of `url`\n",
        "    urls = set()\n",
        "    # domain name of the URL without the protocol\n",
        "    domain_name = urlparse(url).netloc\n",
        "    soup = BeautifulSoup(requests.get(url).content, \"html.parser\")\n",
        "    for a_tag in soup.findAll(\"a\"):\n",
        "        href = a_tag.attrs.get(\"href\")\n",
        "        if href == \"\" or href is None:\n",
        "            # href empty tag\n",
        "            continue\n",
        "        # join the URL if it's relative (not absolute link)\n",
        "        href = urljoin(url, href)\n",
        "        parsed_href = urlparse(href)\n",
        "        # remove URL GET parameters, URL fragments, etc.\n",
        "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
        "        if not is_valid(href):\n",
        "            # not a valid URL\n",
        "            continue\n",
        "        if href in internal_urls:\n",
        "            # already in the set\n",
        "            continue\n",
        "        if domain_name not in href:\n",
        "            # external link\n",
        "            if href not in external_urls:\n",
        "                print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
        "                external_urls.add(href)\n",
        "            continue\n",
        "        print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
        "        urls.add(href)\n",
        "        internal_urls.add(href)\n",
        "    return urls\n",
        "\n",
        "\n",
        "def crawl(url, max_urls=30):\n",
        "    \"\"\"\n",
        "    Crawls a web page and extracts all links.\n",
        "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
        "    params:\n",
        "        max_urls (int): number of max urls to crawl, default is 30.\n",
        "    \"\"\"\n",
        "    global total_urls_visited\n",
        "    total_urls_visited += 1\n",
        "    print(f\"{YELLOW}[*] Crawling: {url}{RESET}\")\n",
        "    links = get_all_website_links(url)\n",
        "    for link in links:\n",
        "        if total_urls_visited > max_urls:\n",
        "            break\n",
        "        crawl(link, max_urls=max_urls)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser(description=\"Link Extractor Tool with Python\")\n",
        "    parser.add_argument(\"url\", help=\"The URL to extract links from.\")\n",
        "    parser.add_argument(\"-m\", \"--max-urls\", help=\"Number of max URLs to crawl, default is 30.\", default=30, type=int)\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    url = args.url\n",
        "    max_urls = args.max_urls\n",
        "\n",
        "    crawl(url, max_urls=max_urls)\n",
        "\n",
        "    print(\"[+] Total Internal links:\", len(internal_urls))\n",
        "    print(\"[+] Total External links:\", len(external_urls))\n",
        "    print(\"[+] Total URLs:\", len(external_urls) + len(internal_urls))\n",
        "    print(\"[+] Total crawled URLs:\", max_urls)\n",
        "\n",
        "    domain_name = urlparse(url).netloc\n",
        "\n",
        "    # save the internal links to a file\n",
        "    with open(f\"{domain_name}_internal_links.txt\", \"w\") as f:\n",
        "        for internal_link in internal_urls:\n",
        "            print(internal_link.strip(), file=f)\n",
        "\n",
        "    # save the external links to a file\n",
        "    with open(f\"{domain_name}_external_links.txt\", \"w\") as f:\n",
        "        for external_link in external_urls:\n",
        "            print(external_link.strip(), file=f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC0dIxpLnyuy",
        "outputId": "4da3e8af-02a2-47d1-c79d-d711102cacca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [-m MAX_URLS] url\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8M27BVgMoGPt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}